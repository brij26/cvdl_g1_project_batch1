{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '281.70000076293945'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 90\u001b[0m\n\u001b[0;32m     87\u001b[0m                 f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_center\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_center\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwidth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Process train, val, and test sets\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m \u001b[43mprocess_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m process_split(val_files, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m process_split(test_files, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 54\u001b[0m, in \u001b[0;36mprocess_split\u001b[1;34m(file_list, split_name)\u001b[0m\n\u001b[0;32m     52\u001b[0m xml_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ANNOTATIONS_DIR, file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(IMAGES_DIR, file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 54\u001b[0m objects, filename \u001b[38;5;241m=\u001b[39m \u001b[43mparse_voc_xml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxml_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m img_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(coco_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     57\u001b[0m coco_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: img_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m500\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m500\u001b[39m})\n",
      "Cell \u001b[1;32mIn[1], line 38\u001b[0m, in \u001b[0;36mparse_voc_xml\u001b[1;34m(xml_file)\u001b[0m\n\u001b[0;32m     36\u001b[0m bbox \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbndbox\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m xmin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(bbox\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxmin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m---> 38\u001b[0m ymin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mymin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m xmax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(bbox\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m     40\u001b[0m ymax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(bbox\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mymax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '281.70000076293945'"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import xml.etree.ElementTree as ET\n",
    "# import random\n",
    "# from shutil import copy2\n",
    "\n",
    "# # Paths\n",
    "# VOC_ROOT = \"VOC2012\"\n",
    "# ANNOTATIONS_DIR = os.path.join(VOC_ROOT, \"Annotations\")\n",
    "# IMAGES_DIR = os.path.join(VOC_ROOT, \"JPEGImages\")\n",
    "# IMAGESETS_DIR = os.path.join(VOC_ROOT, \"ImageSets\", \"Main\")\n",
    "# OUTPUT_DIR = \"output_dataset\"\n",
    "\n",
    "# # Create output directories\n",
    "# os.makedirs(os.path.join(OUTPUT_DIR, \"COCO\", \"annotations\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(OUTPUT_DIR, \"YOLO\", \"labels\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(OUTPUT_DIR, \"images\", \"train\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(OUTPUT_DIR, \"images\", \"val\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(OUTPUT_DIR, \"images\", \"test\"), exist_ok=True)\n",
    "\n",
    "# # Read image file names\n",
    "# all_images = [f.replace(\".xml\", \"\") for f in os.listdir(ANNOTATIONS_DIR) if f.endswith(\".xml\")]\n",
    "# random.shuffle(all_images)\n",
    "\n",
    "# # Split dataset\n",
    "# train_split = int(0.7 * len(all_images))\n",
    "# val_split = int(0.9 * len(all_images))\n",
    "# train_files, val_files, test_files = all_images[:train_split], all_images[train_split:val_split], all_images[val_split:]\n",
    "\n",
    "# def parse_voc_xml(xml_file):\n",
    "#     tree = ET.parse(xml_file)\n",
    "#     root = tree.getroot()\n",
    "#     objects = []\n",
    "#     for obj in root.findall(\"object\"):\n",
    "#         name = obj.find(\"name\").text\n",
    "#         bbox = obj.find(\"bndbox\")\n",
    "#         xmin = int(bbox.find(\"xmin\").text)\n",
    "#         ymin = int(bbox.find(\"ymin\").text)\n",
    "#         xmax = int(bbox.find(\"xmax\").text)\n",
    "#         ymax = int(bbox.find(\"ymax\").text)\n",
    "#         objects.append({\"name\": name, \"bbox\": [xmin, ymin, xmax, ymax]})\n",
    "#     return objects, root.find(\"filename\").text\n",
    "\n",
    "# # Create COCO JSON format\n",
    "# coco_dataset = {\"images\": [], \"annotations\": [], \"categories\": []}\n",
    "# category_map = {}\n",
    "# ann_id = 0\n",
    "\n",
    "# def process_split(file_list, split_name):\n",
    "#     global ann_id\n",
    "#     for file in file_list:\n",
    "#         xml_path = os.path.join(ANNOTATIONS_DIR, file + \".xml\")\n",
    "#         image_path = os.path.join(IMAGES_DIR, file + \".jpg\")\n",
    "#         objects, filename = parse_voc_xml(xml_path)\n",
    "#         img_id = len(coco_dataset[\"images\"])\n",
    "        \n",
    "#         coco_dataset[\"images\"].append({\"id\": img_id, \"file_name\": filename, \"height\": 500, \"width\": 500})\n",
    "        \n",
    "#         for obj in objects:\n",
    "#             if obj[\"name\"] not in category_map:\n",
    "#                 category_map[obj[\"name\"]] = len(category_map) + 1\n",
    "#                 coco_dataset[\"categories\"].append({\"id\": category_map[obj[\"name\"]], \"name\": obj[\"name\"]})\n",
    "            \n",
    "#             bbox = obj[\"bbox\"]\n",
    "#             coco_dataset[\"annotations\"].append({\n",
    "#                 \"id\": ann_id,\n",
    "#                 \"image_id\": img_id,\n",
    "#                 \"category_id\": category_map[obj[\"name\"]],\n",
    "#                 \"bbox\": [bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]],\n",
    "#                 \"area\": (bbox[2] - bbox[0]) * (bbox[3] - bbox[1]),\n",
    "#                 \"iscrowd\": 0\n",
    "#             })\n",
    "#             ann_id += 1\n",
    "        \n",
    "#         # Copy image to respective folder\n",
    "#         copy2(image_path, os.path.join(OUTPUT_DIR, \"images\", split_name, filename))\n",
    "        \n",
    "#         # Create YOLO labels\n",
    "#         yolo_label_path = os.path.join(OUTPUT_DIR, \"YOLO\", \"labels\", file + \".txt\")\n",
    "#         with open(yolo_label_path, \"w\") as f:\n",
    "#             for obj in objects:\n",
    "#                 class_id = category_map[obj[\"name\"]] - 1\n",
    "#                 x_center = (obj[\"bbox\"][0] + obj[\"bbox\"][2]) / 2 / 500\n",
    "#                 y_center = (obj[\"bbox\"][1] + obj[\"bbox\"][3]) / 2 / 500\n",
    "#                 width = (obj[\"bbox\"][2] - obj[\"bbox\"][0]) / 500\n",
    "#                 height = (obj[\"bbox\"][3] - obj[\"bbox\"][1]) / 500\n",
    "#                 f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
    "\n",
    "# # Process train, val, and test sets\n",
    "# process_split(train_files, \"train\")\n",
    "# process_split(val_files, \"val\")\n",
    "# process_split(test_files, \"test\")\n",
    "\n",
    "# # Save COCO JSON\n",
    "# with open(os.path.join(OUTPUT_DIR, \"COCO\", \"annotations\", \"instances.json\"), \"w\") as f:\n",
    "#     json.dump(coco_dataset, f, indent=4)\n",
    "\n",
    "# print(\"Conversion complete! COCO JSON and YOLO TXT annotations are ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conversion complete! COCO JSON and YOLO TXT annotations are ready.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "from shutil import copy2\n",
    "\n",
    "# Paths\n",
    "VOC_ROOT = \"VOC2012\"\n",
    "ANNOTATIONS_DIR = os.path.join(VOC_ROOT, \"Annotations\")\n",
    "IMAGES_DIR = os.path.join(VOC_ROOT, \"JPEGImages\")\n",
    "OUTPUT_DIR = \"output_dataset\"\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"COCO\", \"annotations\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"YOLO\", \"labels\", \"train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"YOLO\", \"labels\", \"val\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"YOLO\", \"labels\", \"test\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"images\", \"train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"images\", \"val\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"images\", \"test\"), exist_ok=True)\n",
    "\n",
    "# Read all annotation file names\n",
    "all_images = [f.replace(\".xml\", \"\") for f in os.listdir(ANNOTATIONS_DIR) if f.endswith(\".xml\")]\n",
    "random.shuffle(all_images)\n",
    "\n",
    "# Split dataset\n",
    "train_split = int(0.7 * len(all_images))\n",
    "val_split = int(0.9 * len(all_images))\n",
    "train_files = all_images[:train_split]\n",
    "val_files = all_images[train_split:val_split]\n",
    "test_files = all_images[val_split:]\n",
    "\n",
    "# Function to parse Pascal VOC XML annotation files\n",
    "def parse_voc_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    objects = []\n",
    "    for obj in root.findall(\"object\"):\n",
    "        name = obj.find(\"name\").text\n",
    "        bbox = obj.find(\"bndbox\")\n",
    "        xmin = float(bbox.find(\"xmin\").text)\n",
    "        ymin = float(bbox.find(\"ymin\").text)\n",
    "        xmax = float(bbox.find(\"xmax\").text)\n",
    "        ymax = float(bbox.find(\"ymax\").text)\n",
    "        objects.append({\"name\": name, \"bbox\": [xmin, ymin, xmax, ymax]})\n",
    "    return objects, root.find(\"filename\").text, int(root.find(\"size/width\").text), int(root.find(\"size/height\").text)\n",
    "\n",
    "\n",
    "## output of this function\n",
    "\n",
    "#     (\n",
    "#       [{'name': 'dog', 'bbox': [34.0, 50.0, 150.0, 200.0]}, \n",
    "#       {'name': 'person', 'bbox': [100.0, 120.0, 180.0, 300.0]}],\n",
    "#           'image123.jpg',\n",
    "#           500,\n",
    "#           375\n",
    "#    \n",
    "\n",
    "# Create COCO annotations\n",
    "def create_coco_annotation():\n",
    "    return {\"images\": [], \"annotations\": [], \"categories\": []}\n",
    "\n",
    "coco_train = create_coco_annotation()\n",
    "coco_val = create_coco_annotation()\n",
    "coco_test = create_coco_annotation()\n",
    "category_map = {}\n",
    "ann_id = 0\n",
    "\n",
    "# Process each dataset split\n",
    "def process_split(file_list, split_name, coco_dataset):\n",
    "    global ann_id\n",
    "    \n",
    "    for file in file_list:\n",
    "        xml_path = os.path.join(ANNOTATIONS_DIR, file + \".xml\")\n",
    "        image_path = os.path.join(IMAGES_DIR, file + \".jpg\")\n",
    "\n",
    "        if not os.path.exists(xml_path) or not os.path.exists(image_path):\n",
    "            print(f\"Warning: {file} annotation or image not found, skipping...\")\n",
    "            continue\n",
    "\n",
    "        objects, filename, img_width, img_height = parse_voc_xml(xml_path)\n",
    "        img_id = len(coco_dataset[\"images\"])  # Unique image ID for each split\n",
    "\n",
    "        # Add image details to COCO dataset\n",
    "        coco_dataset[\"images\"].append({\n",
    "            \"id\": img_id,\n",
    "            \"file_name\": filename,\n",
    "            \"height\": img_height,\n",
    "            \"width\": img_width\n",
    "        })\n",
    "\n",
    "        for obj in objects:\n",
    "            if obj[\"name\"] not in category_map:\n",
    "                category_map[obj[\"name\"]] = len(category_map) + 1\n",
    "                coco_train[\"categories\"].append({\"id\": category_map[obj[\"name\"]], \"name\": obj[\"name\"]})\n",
    "                coco_val[\"categories\"].append({\"id\": category_map[obj[\"name\"]], \"name\": obj[\"name\"]})\n",
    "                coco_test[\"categories\"].append({\"id\": category_map[obj[\"name\"]], \"name\": obj[\"name\"]})\n",
    "\n",
    "            bbox = obj[\"bbox\"]\n",
    "            coco_dataset[\"annotations\"].append({\n",
    "                \"id\": ann_id,\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": category_map[obj[\"name\"]],\n",
    "                \"bbox\": [bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]],\n",
    "                \"area\": (bbox[2] - bbox[0]) * (bbox[3] - bbox[1]),\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            ann_id += 1\n",
    "\n",
    "        # Copy image to respective split folder\n",
    "        copy2(image_path, os.path.join(OUTPUT_DIR, \"images\", split_name, filename))\n",
    "\n",
    "        # Save YOLO label\n",
    "        yolo_label_path = os.path.join(OUTPUT_DIR, \"YOLO\", \"labels\", split_name, file + \".txt\")\n",
    "        with open(yolo_label_path, \"w\") as f:\n",
    "            for obj in objects:\n",
    "                class_id = category_map[obj[\"name\"]] - 1\n",
    "                x_center = (obj[\"bbox\"][0] + obj[\"bbox\"][2]) / 2 / img_width\n",
    "                y_center = (obj[\"bbox\"][1] + obj[\"bbox\"][3]) / 2 / img_height\n",
    "                width = (obj[\"bbox\"][2] - obj[\"bbox\"][0]) / img_width\n",
    "                height = (obj[\"bbox\"][3] - obj[\"bbox\"][1]) / img_height\n",
    "                f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "\n",
    "# Process each split\n",
    "process_split(train_files, \"train\", coco_train)\n",
    "process_split(val_files, \"val\", coco_val)\n",
    "process_split(test_files, \"test\", coco_test)\n",
    "\n",
    "# Save COCO JSON annotations\n",
    "with open(os.path.join(OUTPUT_DIR, \"COCO\", \"annotations\", \"instances_train.json\"), \"w\") as f:\n",
    "    json.dump(coco_train, f, indent=4)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"COCO\", \"annotations\", \"instances_val.json\"), \"w\") as f:\n",
    "    json.dump(coco_val, f, indent=4)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"COCO\", \"annotations\", \"instances_test.json\"), \"w\") as f:\n",
    "    json.dump(coco_test, f, indent=4)\n",
    "\n",
    "print(\"✅ Conversion complete! COCO JSON and YOLO TXT annotations are ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Train: Images = 11987 | YOLO Labels = 11987 | COCO exists = True\n",
      "🔹 Val: Images = 3425 | YOLO Labels = 3425 | COCO exists = True\n",
      "🔹 Test: Images = 1713 | YOLO Labels = 1713 | COCO exists = True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"output_dataset\"\n",
    "\n",
    "# Check image splits\n",
    "train_images = os.listdir(os.path.join(output_dir, \"images\", \"train\"))\n",
    "val_images = os.listdir(os.path.join(output_dir, \"images\", \"val\"))\n",
    "test_images = os.listdir(os.path.join(output_dir, \"images\", \"test\"))\n",
    "\n",
    "# Check YOLO labels\n",
    "train_labels_yolo = os.listdir(os.path.join(output_dir, \"YOLO\", \"labels\", \"train\"))\n",
    "val_labels_yolo = os.listdir(os.path.join(output_dir, \"YOLO\", \"labels\", \"val\"))\n",
    "test_labels_yolo = os.listdir(os.path.join(output_dir, \"YOLO\", \"labels\", \"test\"))\n",
    "\n",
    "# Check COCO annotation files\n",
    "train_coco = os.path.exists(os.path.join(output_dir, \"COCO\", \"annotations\", \"instances_train.json\"))\n",
    "val_coco = os.path.exists(os.path.join(output_dir, \"COCO\", \"annotations\", \"instances_val.json\"))\n",
    "test_coco = os.path.exists(os.path.join(output_dir, \"COCO\", \"annotations\", \"instances_test.json\"))\n",
    "\n",
    "print(\"🔹 Train: Images =\", len(train_images), \"| YOLO Labels =\", len(train_labels_yolo), \"| COCO exists =\", train_coco)\n",
    "print(\"🔹 Val: Images =\", len(val_images), \"| YOLO Labels =\", len(val_labels_yolo), \"| COCO exists =\", val_coco)\n",
    "print(\"🔹 Test: Images =\", len(test_images), \"| YOLO Labels =\", len(test_labels_yolo), \"| COCO exists =\", test_coco)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train set is consistent between images and YOLO labels.\n",
      "✅ Val set is consistent between images and YOLO labels.\n",
      "✅ Test set is consistent between images and YOLO labels.\n"
     ]
    }
   ],
   "source": [
    "train_images_set = set(train_images)\n",
    "train_labels_yolo_set = set([f.replace(\".txt\", \".jpg\") for f in train_labels_yolo])\n",
    "\n",
    "if train_images_set == train_labels_yolo_set:\n",
    "    print(\"✅ Train set is consistent between images and YOLO labels.\")\n",
    "else:\n",
    "    print(\"❌ Train set mismatch!\")\n",
    "\n",
    "val_images_set = set(val_images)\n",
    "val_labels_yolo_set = set([f.replace(\".txt\", \".jpg\") for f in val_labels_yolo])\n",
    "\n",
    "if val_images_set == val_labels_yolo_set:\n",
    "    print(\"✅ Val set is consistent between images and YOLO labels.\")\n",
    "else:\n",
    "    print(\"❌ Val set mismatch!\")\n",
    "\n",
    "test_images_set = set(test_images)\n",
    "test_labels_yolo_set = set([f.replace(\".txt\", \".jpg\") for f in test_labels_yolo])\n",
    "\n",
    "if test_images_set == test_labels_yolo_set:\n",
    "    print(\"✅ Test set is consistent between images and YOLO labels.\")\n",
    "else:\n",
    "    print(\"❌ Test set mismatch!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ COCO and YOLO train labels contain the same images.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "output_dir = \"output_dataset\"\n",
    "coco_train_json = os.path.join(output_dir, \"COCO\", \"annotations\", \"instances_train.json\")\n",
    "yolo_labels_dir = os.path.join(output_dir, \"YOLO\", \"labels\", \"train\")\n",
    "\n",
    "# Load COCO JSON\n",
    "with open(coco_train_json, \"r\") as f:\n",
    "    coco_train_data = json.load(f)\n",
    "\n",
    "# Extract image filenames from COCO JSON\n",
    "coco_train_images = {img[\"file_name\"].replace(\".jpg\", \"\") for img in coco_train_data[\"images\"]}\n",
    "\n",
    "# Extract image names from YOLO labels (remove .txt extension)\n",
    "yolo_train_images = {f.replace(\".txt\", \"\") for f in os.listdir(yolo_labels_dir) if f.endswith(\".txt\")}\n",
    "\n",
    "# Check if both sets match\n",
    "if coco_train_images == yolo_train_images:\n",
    "    print(\"✅ COCO and YOLO train labels contain the same images.\")\n",
    "else:\n",
    "    print(\"❌ Mismatch detected!\")\n",
    "    print(\"👉 Images in COCO but missing in YOLO:\", coco_train_images - yolo_train_images)\n",
    "    print(\"👉 Images in YOLO but missing in COCO:\", yolo_train_images - coco_train_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define original paths\n",
    "base_path = \"/kaggle/input/g2-project/output_dataset\"\n",
    "image_dirs = {\n",
    "    \"train\": os.path.join(base_path, \"images/train\"),\n",
    "    \"val\": os.path.join(base_path, \"images/val\"),\n",
    "    \"test\": os.path.join(base_path, \"images/test\"),\n",
    "}\n",
    "label_dirs = {\n",
    "    \"train\": os.path.join(base_path, \"YOLO/labels/train\"),\n",
    "    \"val\": os.path.join(base_path, \"YOLO/labels/val\"),\n",
    "    \"test\": os.path.join(base_path, \"YOLO/labels/test\"),\n",
    "}\n",
    "\n",
    "# Define new YOLO-structured dataset path\n",
    "output_base = \"/kaggle/working/YOLO_dataset\"\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    img_output = os.path.join(output_base, split, \"images\")\n",
    "    lbl_output = os.path.join(output_base, split, \"labels\")\n",
    "    \n",
    "    os.makedirs(img_output, exist_ok=True)\n",
    "    os.makedirs(lbl_output, exist_ok=True)\n",
    "\n",
    "    # Move images\n",
    "    for file in os.listdir(image_dirs[split]):\n",
    "        src = os.path.join(image_dirs[split], file)\n",
    "        dst = os.path.join(img_output, file)\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "    # Move labels\n",
    "    for file in os.listdir(label_dirs[split]):\n",
    "        src = os.path.join(label_dirs[split], file)\n",
    "        dst = os.path.join(lbl_output, file)\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "print(\"Dataset successfully reorganized into:\", output_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
