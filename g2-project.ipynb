{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-31T16:47:47.552020Z",
     "iopub.status.busy": "2025-03-31T16:47:47.551665Z",
     "iopub.status.idle": "2025-03-31T16:50:35.141261Z",
     "shell.execute_reply": "2025-03-31T16:50:35.140345Z",
     "shell.execute_reply.started": "2025-03-31T16:47:47.551993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP-1 :- `dataset preparartion`\n",
    "\n",
    "*i already have a train, test, and validation split in my current dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T16:50:35.142785Z",
     "iopub.status.busy": "2025-03-31T16:50:35.142456Z",
     "iopub.status.idle": "2025-03-31T16:50:35.146041Z",
     "shell.execute_reply": "2025-03-31T16:50:35.145317Z",
     "shell.execute_reply.started": "2025-03-31T16:50:35.142750Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import shutil\n",
    "# import random\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T16:50:35.147823Z",
     "iopub.status.busy": "2025-03-31T16:50:35.147616Z",
     "iopub.status.idle": "2025-03-31T16:50:35.157658Z",
     "shell.execute_reply": "2025-03-31T16:50:35.156783Z",
     "shell.execute_reply.started": "2025-03-31T16:50:35.147807Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Define paths\n",
    "# BASE_DIR = Path(\"/kaggle/input/coco-2017-dataset\")\n",
    "# OUTPUT_DIR = Path(\"/kaggle/working/coco_prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T16:50:35.159136Z",
     "iopub.status.busy": "2025-03-31T16:50:35.158872Z",
     "iopub.status.idle": "2025-03-31T16:50:35.172000Z",
     "shell.execute_reply": "2025-03-31T16:50:35.171266Z",
     "shell.execute_reply.started": "2025-03-31T16:50:35.159118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ANNOTATIONS_FILE = BASE_DIR / \"coco2017/annotations/instances_train2017.json\"\n",
    "# IMAGES_DIR = BASE_DIR / \"coco2017/train2017\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T16:50:35.173045Z",
     "iopub.status.busy": "2025-03-31T16:50:35.172748Z",
     "iopub.status.idle": "2025-03-31T16:50:35.184426Z",
     "shell.execute_reply": "2025-03-31T16:50:35.183638Z",
     "shell.execute_reply.started": "2025-03-31T16:50:35.173017Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Create output directories\n",
    "# (OUTPUT_DIR / \"train\" / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "# (OUTPUT_DIR / \"val\" / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "# (OUTPUT_DIR / \"test\" / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "# (OUTPUT_DIR / \"train\" / \"labels\").mkdir(parents=True, exist_ok=True)\n",
    "# (OUTPUT_DIR / \"val\" / \"labels\").mkdir(parents=True, exist_ok=True)\n",
    "# (OUTPUT_DIR / \"test\" / \"labels\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T16:50:35.185291Z",
     "iopub.status.busy": "2025-03-31T16:50:35.185073Z",
     "iopub.status.idle": "2025-03-31T16:50:35.195488Z",
     "shell.execute_reply": "2025-03-31T16:50:35.194710Z",
     "shell.execute_reply.started": "2025-03-31T16:50:35.185272Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Load COCO annotations\n",
    "# def load_coco_annotations():\n",
    "#     with open(ANNOTATIONS_FILE, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T16:50:35.196575Z",
     "iopub.status.busy": "2025-03-31T16:50:35.196324Z",
     "iopub.status.idle": "2025-03-31T16:50:35.206087Z",
     "shell.execute_reply": "2025-03-31T16:50:35.205380Z",
     "shell.execute_reply.started": "2025-03-31T16:50:35.196556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def split_dataset(image_ids, train_ratio=0.7, val_ratio=0.2):\n",
    "#     random.shuffle(image_ids)\n",
    "#     train_split = int(len(image_ids) * train_ratio)\n",
    "#     val_split = int(len(image_ids) * (train_ratio + val_ratio))\n",
    "#     return image_ids[:train_split], image_ids[train_split:val_split], image_ids[val_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T16:50:35.208445Z",
     "iopub.status.busy": "2025-03-31T16:50:35.208246Z",
     "iopub.status.idle": "2025-03-31T16:50:35.216838Z",
     "shell.execute_reply": "2025-03-31T16:50:35.216046Z",
     "shell.execute_reply.started": "2025-03-31T16:50:35.208428Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def convert_coco_to_yolo(annotations, image_id, image_info, output_dir):\n",
    "#     img_width, img_height = image_info['width'], image_info['height']\n",
    "#     label_file = output_dir / f\"{image_id:012d}.txt\"\n",
    "#     with open(label_file, 'w') as f:\n",
    "#         for ann in annotations:\n",
    "#             category_id = ann['category_id'] - 1  # Adjust class index (COCO classes start at 1)\n",
    "#             bbox = ann['bbox']\n",
    "#             x_center = (bbox[0] + bbox[2] / 2) / img_width\n",
    "#             y_center = (bbox[1] + bbox[3] / 2) / img_height\n",
    "#             width = bbox[2] / img_width\n",
    "#             height = bbox[3] / img_height\n",
    "#             f.write(f\"{category_id} {x_center} {y_center} {width} {height}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T16:50:35.218251Z",
     "iopub.status.busy": "2025-03-31T16:50:35.217961Z",
     "iopub.status.idle": "2025-03-31T16:50:35.228998Z",
     "shell.execute_reply": "2025-03-31T16:50:35.228384Z",
     "shell.execute_reply.started": "2025-03-31T16:50:35.218232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def process_coco():\n",
    "#     coco_data = load_coco_annotations()\n",
    "#     image_id_map = {img['id']: img for img in coco_data['images']}\n",
    "#     image_annotations = {}\n",
    "    \n",
    "#     for ann in coco_data['annotations']:\n",
    "#         image_id = ann['image_id']\n",
    "#         if image_id not in image_annotations:\n",
    "#             image_annotations[image_id] = []\n",
    "#         image_annotations[image_id].append(ann)\n",
    "    \n",
    "#     train_ids, val_ids, test_ids = split_dataset(list(image_id_map.keys()))\n",
    "#     dataset_splits = {\"train\": train_ids, \"val\": val_ids, \"test\": test_ids}\n",
    "    \n",
    "#     for split, image_ids in dataset_splits.items():\n",
    "#         img_output_dir = OUTPUT_DIR / split / \"images\"\n",
    "#         label_output_dir = OUTPUT_DIR / split / \"labels\"\n",
    "        \n",
    "#         for image_id in tqdm(image_ids, desc=f\"Processing {split}\"):\n",
    "#             image_info = image_id_map[image_id]\n",
    "#             image_file = IMAGES_DIR / image_info['file_name']\n",
    "#             shutil.copy(image_file, img_output_dir / image_info['file_name'])\n",
    "            \n",
    "#             if image_id in image_annotations:\n",
    "#                 convert_coco_to_yolo(image_annotations[image_id], image_id, image_info, label_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T16:50:35.229954Z",
     "iopub.status.busy": "2025-03-31T16:50:35.229717Z",
     "iopub.status.idle": "2025-03-31T16:50:35.242123Z",
     "shell.execute_reply": "2025-03-31T16:50:35.241427Z",
     "shell.execute_reply.started": "2025-03-31T16:50:35.229918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# process_coco()\n",
    "# print(\"Dataset preparation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T16:50:35.243082Z",
     "iopub.status.busy": "2025-03-31T16:50:35.242804Z",
     "iopub.status.idle": "2025-03-31T16:50:35.252599Z",
     "shell.execute_reply": "2025-03-31T16:50:35.251778Z",
     "shell.execute_reply.started": "2025-03-31T16:50:35.243056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !zip -r /kaggle/working/coco_prepared.zip /kaggle/working/coco_prepared/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T16:50:35.253760Z",
     "iopub.status.busy": "2025-03-31T16:50:35.253485Z",
     "iopub.status.idle": "2025-03-31T16:50:35.263467Z",
     "shell.execute_reply": "2025-03-31T16:50:35.262761Z",
     "shell.execute_reply.started": "2025-03-31T16:50:35.253733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade pydrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T16:50:35.264391Z",
     "iopub.status.busy": "2025-03-31T16:50:35.264197Z",
     "iopub.status.idle": "2025-03-31T16:50:35.271506Z",
     "shell.execute_reply": "2025-03-31T16:50:35.270931Z",
     "shell.execute_reply.started": "2025-03-31T16:50:35.264374Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# json_key_path = \"/kaggle/input/my-json/cvdlg2project-29a329abe8c2.json\"\n",
    "\n",
    "# with open(json_key_path, \"r\") as json_file:\n",
    "#     json_key_dict = json.load(json_file)  # Correctly loads the JSON file as a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T16:50:35.272585Z",
     "iopub.status.busy": "2025-03-31T16:50:35.272280Z",
     "iopub.status.idle": "2025-03-31T16:50:35.283650Z",
     "shell.execute_reply": "2025-03-31T16:50:35.282962Z",
     "shell.execute_reply.started": "2025-03-31T16:50:35.272558Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from googleapiclient.discovery import build\n",
    "# from google.oauth2 import service_account\n",
    "\n",
    "# credentials = service_account.Credentials.from_service_account_info(json_key_dict)\n",
    "# drive_service = build(\"drive\", \"v3\", credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T16:53:01.989315Z",
     "iopub.status.busy": "2025-03-31T16:53:01.989050Z",
     "iopub.status.idle": "2025-03-31T16:53:01.992574Z",
     "shell.execute_reply": "2025-03-31T16:53:01.991669Z",
     "shell.execute_reply.started": "2025-03-31T16:53:01.989296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from googleapiclient.http import MediaFileUpload\n",
    "# import os\n",
    "\n",
    "# # Define the Google Drive folder ID (OPTIONAL: If you want to upload to a specific folder)\n",
    "# FOLDER_ID = \"1FWV8Dtqeky-8wB4lN6nvNDocDiVc7E-b\"  # Change this if you have a specific Drive folder\n",
    "\n",
    "# # Define the directory containing your dataset\n",
    "# LOCAL_DATASET_DIR = \"/kaggle/working/coco_prepared\"\n",
    "\n",
    "# # Function to upload a file to Google Drive\n",
    "# def upload_file(file_path, drive_service, folder_id=None):\n",
    "#     file_metadata = {\n",
    "#         \"name\": os.path.basename(file_path),\n",
    "#         \"parents\": [folder_id] if folder_id else []\n",
    "#     }\n",
    "#     media = MediaFileUpload(file_path, resumable=True)\n",
    "    \n",
    "#     file = drive_service.files().create(\n",
    "#         body=file_metadata,\n",
    "#         media_body=media,\n",
    "#         fields=\"id\"\n",
    "#     ).execute()\n",
    "\n",
    "#     print(f\"Uploaded {file_path} to Google Drive with file ID: {file.get('id')}\")\n",
    "\n",
    "# # Upload all files in the dataset directory\n",
    "# for root, _, files in os.walk(LOCAL_DATASET_DIR):\n",
    "#     for file in files:\n",
    "#         file_path = os.path.join(root, file)\n",
    "#         upload_file(file_path, drive_service, FOLDER_ID)\n",
    "\n",
    "# print(\" Dataset uploaded to Google Drive successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `creating dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. for Faster RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faster R-CNN dataset created with 70% train, 20% val, 10% test split!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "dataset_dir = r\"D:\\cvdl_project_g2\\data_ingension\\coco2017\"\n",
    "train_annotations_file = r\"D:\\cvdl_project_g2\\data_ingension\\coco2017\\annotations\\instances_train2017.json\"\n",
    "train_image_dir = r\"D:\\cvdl_project_g2\\data_ingension\\coco2017\\train2017\"\n",
    "\n",
    "def ensure_dir_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "output_dir_frcnn = r\"D:\\cvdl_project_g2\\faster_rcnn_dataset\"  # Change to your existing folder\n",
    "output_images_dir_frcnn = os.path.join(output_dir_frcnn, \"images\")\n",
    "output_annotations_dir_frcnn = os.path.join(output_dir_frcnn, \"annotations\")\n",
    "\n",
    "# Ensure directories exist inside the existing folder\n",
    "ensure_dir_exists(os.path.join(output_images_dir_frcnn, \"train\"))\n",
    "ensure_dir_exists(os.path.join(output_images_dir_frcnn, \"val\"))\n",
    "ensure_dir_exists(os.path.join(output_images_dir_frcnn, \"test\"))\n",
    "ensure_dir_exists(output_annotations_dir_frcnn)\n",
    "\n",
    "# Load COCO annotations\n",
    "with open(train_annotations_file, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Shuffle images for dataset split\n",
    "all_images = coco_data[\"images\"]\n",
    "random.shuffle(all_images)\n",
    "\n",
    "# Split dataset (70% train, 20% val, 10% test)\n",
    "total_images = len(all_images)\n",
    "train_split = int(0.7 * total_images)\n",
    "val_split = int(0.2 * total_images)\n",
    "\n",
    "train_images = all_images[:train_split]\n",
    "val_images = all_images[train_split:train_split + val_split]\n",
    "test_images = all_images[train_split + val_split:]\n",
    "\n",
    "# Create Faster R-CNN dataset (images and annotations in COCO format)\n",
    "def create_frcnn_dataset(image_list, split_name):\n",
    "    new_coco_data = {\n",
    "        \"info\": coco_data[\"info\"],\n",
    "        \"licenses\": coco_data[\"licenses\"],\n",
    "        \"categories\": coco_data[\"categories\"],\n",
    "        \"images\": [],\n",
    "        \"annotations\": []\n",
    "    }\n",
    "\n",
    "    for image in image_list:\n",
    "        image_id = image[\"id\"]\n",
    "        image_filename = image[\"file_name\"]\n",
    "\n",
    "        # Copy image to Faster R-CNN directory\n",
    "        src_img_path = os.path.join(train_image_dir, image_filename)\n",
    "        dst_img_path = os.path.join(output_images_dir_frcnn, split_name, image_filename)\n",
    "        shutil.copy(src_img_path, dst_img_path)\n",
    "\n",
    "        # Add image and annotations to new COCO JSON\n",
    "        new_coco_data[\"images\"].append(image)\n",
    "        annotations = [anno for anno in coco_data[\"annotations\"] if anno[\"image_id\"] == image_id]\n",
    "        new_coco_data[\"annotations\"].extend(annotations)\n",
    "\n",
    "    # Save new COCO annotation file\n",
    "    with open(os.path.join(output_annotations_dir_frcnn, f\"instances_{split_name}.json\"), \"w\") as f:\n",
    "        json.dump(new_coco_data, f, indent=4)\n",
    "\n",
    "# Create dataset for Faster R-CNN\n",
    "create_frcnn_dataset(train_images, \"train\")\n",
    "create_frcnn_dataset(val_images, \"val\")\n",
    "create_frcnn_dataset(test_images, \"test\")\n",
    "\n",
    "print(\"Faster R-CNN dataset created with 70% train, 20% val, 10% test split!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. for YOLOv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv5 dataset created with 70% train, 20% val, 10% test split!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define paths for original COCO dataset\n",
    "dataset_dir = r\"D:\\cvdl_project_g2\\data_ingension\\coco2017\"\n",
    "train_annotations_file = os.path.join(dataset_dir, \"annotations\", \"instances_train2017.json\")\n",
    "train_image_dir = os.path.join(dataset_dir, \"train2017\")\n",
    "\n",
    "def ensure_dir_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# Define output directory for YOLOv5 dataset\n",
    "output_dir_yolo = r\"D:\\cvdl_project_g2\\yolov5_dataset\"\n",
    "output_images_dir_yolo = os.path.join(output_dir_yolo, \"images\")\n",
    "output_labels_dir_yolo = os.path.join(output_dir_yolo, \"labels\")\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    ensure_dir_exists(os.path.join(output_images_dir_yolo, split))\n",
    "    ensure_dir_exists(os.path.join(output_labels_dir_yolo, split))\n",
    "\n",
    "# Load COCO annotations\n",
    "with open(train_annotations_file, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Shuffle images for dataset split\n",
    "all_images = coco_data[\"images\"]\n",
    "random.shuffle(all_images)\n",
    "\n",
    "# Split dataset (70% train, 20% val, 10% test)\n",
    "total_images = len(all_images)\n",
    "train_split = int(0.7 * total_images)\n",
    "val_split = int(0.2 * total_images)\n",
    "\n",
    "train_images = all_images[:train_split]\n",
    "val_images = all_images[train_split:train_split + val_split]\n",
    "test_images = all_images[train_split + val_split:]\n",
    "\n",
    "# Map category IDs to YOLO class indices\n",
    "category_map = {cat[\"id\"]: idx for idx, cat in enumerate(coco_data[\"categories\"])}\n",
    "\n",
    "# Function to normalize bounding boxes for YOLO format\n",
    "def normalize_bbox(bbox, img_width, img_height):\n",
    "    x_min, y_min, width, height = bbox\n",
    "    x_center = (x_min + width / 2) / img_width\n",
    "    y_center = (y_min + height / 2) / img_height\n",
    "    width /= img_width\n",
    "    height /= img_height\n",
    "    return x_center, y_center, width, height\n",
    "\n",
    "# Create YOLOv5 dataset (images and labels in YOLO format)\n",
    "def create_yolo_dataset(image_list, split_name):\n",
    "    for image in image_list:\n",
    "        image_filename = image[\"file_name\"]\n",
    "        image_id = image[\"id\"]\n",
    "        image_width = image[\"width\"]\n",
    "        image_height = image[\"height\"]\n",
    "\n",
    "        annotations = [anno for anno in coco_data[\"annotations\"] if anno[\"image_id\"] == image_id]\n",
    "\n",
    "        # Save YOLO labels\n",
    "        label_file = os.path.join(output_labels_dir_yolo, split_name, os.path.splitext(image_filename)[0] + \".txt\")\n",
    "        with open(label_file, 'w') as f:\n",
    "            for anno in annotations:\n",
    "                class_id = category_map[anno['category_id']]\n",
    "                bbox = normalize_bbox(anno['bbox'], image_width, image_height)\n",
    "                f.write(f\"{class_id} {bbox[0]} {bbox[1]} {bbox[2]} {bbox[3]}\\n\")\n",
    "\n",
    "        # Copy image to YOLO directory\n",
    "        src_img_path = os.path.join(train_image_dir, image_filename)\n",
    "        dst_img_path = os.path.join(output_images_dir_yolo, split_name, image_filename)\n",
    "        \n",
    "        if os.path.exists(src_img_path):  # Ensure the image exists before copying\n",
    "            shutil.copy(src_img_path, dst_img_path)\n",
    "\n",
    "# Create dataset for YOLOv5\n",
    "create_yolo_dataset(train_images, \"train\")\n",
    "create_yolo_dataset(val_images, \"val\")\n",
    "create_yolo_dataset(test_images, \"test\")\n",
    "\n",
    "print(\"YOLOv5 dataset created with 70% train, 20% val, 10% test split!\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
